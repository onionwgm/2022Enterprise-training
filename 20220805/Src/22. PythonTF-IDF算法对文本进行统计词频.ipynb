{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3611b9b-b633-4766-9584-84aa5930036e",
   "metadata": {},
   "source": [
    "## 0. 读入原始的文本\n",
    "说明：  \n",
    "文件中，中国和科学院之间添加了一个空格，是因为，如果不加空格，jieba会把中国科学院分词为一个词，和演练的word中不一致了。 \n",
    "故此，在中国和科学院之间，添加一个空格，目的仅仅是因为保证和演练的word文档中的执行效果一致。  \n",
    "实际工作中，不用这样子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689b4987-fe86-4a2a-9276-558e0742ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我来到北京清华大学\\n他来到了网易杭研大厦\\n小明硕士毕业与中国 科学院\\n我爱北京天安门'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['我来到北京清华大学', '他来到了网易杭研大厦', '小明硕士毕业与中国 科学院', '我爱北京天安门']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只读方式打开utf-8编码的文件\n",
    "with open(\"sentences.txt\", encoding='utf-8', mode='r') as f:\n",
    "    # 读入文件内容到 text 中\n",
    "    text = f.read()\n",
    "    text\n",
    "# 按照换行符切分文本为语句列表\n",
    "sentences = text.splitlines()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3b88f-13ff-4719-b4c0-74ee113e476c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 使用结巴分词对每个语句进行分词（采用精确模式）\n",
    "参考： https://zhuanlan.zhihu.com/p/29747350 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b56aa9-12e9-4586-8f07-ddc8b3555b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我来到北京清华大学'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\txsli\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.763 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'他来到了网易杭研大厦'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'小明硕士毕业与中国 科学院'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'我爱北京天安门'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['我 来到 北京 清华大学', '他 来到 了 网易 杭研 大厦', '小明 硕士 毕业 与 中国   科学院', '我 爱 北京 天安门']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入 jieba\n",
    "# 可能需要安装 jieba 第三方包，可以在 jupyter 的单元格中，运行： ! pip install jieba\n",
    "import jieba\n",
    "# 声明空列表\n",
    "corpus = []\n",
    "# 遍历语句列表中的每一条语句\n",
    "for sentence in sentences:\n",
    "    # 显示每一条语句内容\n",
    "    sentence\n",
    "    # 使用精确模式对一条语句进行分词，返回的是含有分词结果的迭代器\n",
    "    wordlist_iterator=jieba.cut(sentence)\n",
    "    # 将分词结果迭代器中的每个单词后面添加空格，生成一条语句的分词列表，此处是字符串类型\n",
    "    wordlist = ' '.join(wordlist_iterator)\n",
    "    # 将语句分词后的追加空格间隔的分词字符串追加到 corpus 列表中\n",
    "    corpus.append(wordlist)\n",
    "# 输出 语句分词字符串列表\n",
    "corpus\n",
    "\n",
    "# # 自定义文本\n",
    "# corpus = [\"我 来到 北京 清华大学\",  # 第一类文本切词后的结果，词之间以空格隔开  \n",
    "#           \"他 来到 了 网易 杭研 大厦\",  # 第二类文本的切词结果  \n",
    "#           \"小明 硕士 毕业 与 中国 科学院\",  # 第三类文本的切词结果  \n",
    "#           \"我 爱 北京 天安门\"]  # 第四类文本的切词结果  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44438f8-ee36-4d46-88a7-d19a91db432b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. 计算词频矩阵（使用哈工大停用词表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004b2ff6-07b6-4ba0-ad01-6d091b198121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的包\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d065395-86ee-4f55-8867-76ec87874952",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 获取停用词表\n",
    "Python文本分析-常用中文停用词表（Chinese Stop Words） https://blog.csdn.net/purpen/article/details/105468646   \n",
    "最全中文停用词表（可直接复制） https://blog.csdn.net/dilifish/article/details/117885706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070d2af9-d2f7-4351-b73c-0e364dcef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入哈工大停用词表文件\n",
    "stopword_filename = \"hit_stopwords.txt\"\n",
    "# 打开停用词文件指针，指定编码（支持中文）和模式（读入模式）\n",
    "stopword_file = open(stopword_filename, encoding='utf-8', mode='r')\n",
    "# 读入停用词文件的内容\n",
    "stopword_content = stopword_file.read()\n",
    "# 关闭文件指针\n",
    "stopword_file.close()\n",
    "# 输出停用词表文件的内容\n",
    "#stopword_content\n",
    "# 按照换行转换为停用词列表\n",
    "stopword_list = stopword_content.splitlines()\n",
    "# 输出停用词列表\n",
    "#stopword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbc638-baa1-4753-9d27-660f4187d97e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 生成词频矩阵（其实是词袋（word-vec）模型）\n",
    "CountVectorizer介绍 https://zhuanlan.zhihu.com/p/37644086  \n",
    "https://www.qqxiuzi.cn/bianma/zifuji.php  \n",
    "本质：  \n",
    "去掉所有句子中的停用词，多个句子中重复出现的单词只保留一次，  \n",
    "然后按首字的unicode编码顺序排序为一个向量（feature_names）  \n",
    "将所有句子中剩下的非停用词，生成词频矩阵（4行12列的稀疏矩阵），规则为：  \n",
    "行坐标为语句序号，从0开始，本例为4句话，故为： 0到3  \n",
    "列坐标是本行语句中非停用词对应向量（feature_names）的位置（从0开始，本例共12个唯一的非停用词，故为：0到11），如果该位置有本句的单词则值为1，否则为0。  \n",
    "故此是稀疏矩阵，并用稀疏矩阵的表示形式输出。  \n",
    "**可以这么理解，本例中，一个句子就是一篇文章，所有的四个句子就是所有的文章。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cc8400-8a3f-43dc-967c-c98f178ead8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\Python\\venvs\\work\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['中国', '北京', '大厦', '天安门', '小明', '来到', '杭研', '毕业', '清华大学', '硕士',\n",
       "       '科学院', '网易'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 10)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 3)\t1\n",
      "[[0 1 0 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 1 1 0 0 0 0 1]\n",
      " [1 0 0 0 1 0 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 将文本中的词语转换为词频矩阵\n",
    "# 将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频 \n",
    "# 创建词袋数据结构\n",
    "# 此处指定使用停用词列表作为停词表\n",
    "# 也可以不使用，而是参考max_df设置，默认为1，以根据术语内部语料库文档频率自动检测和过滤停用词。\n",
    "vectorizer=CountVectorizer(stop_words=stopword_list) \n",
    "# 对 语句分词字符串列表-corpus 进行训练和转换，生成词频矩阵\n",
    "vectors=vectorizer.fit_transform(corpus)\n",
    "# 输出 词频矩阵的数据结构\n",
    "vectors\n",
    "# 输出 语句分词字符串列表 中的唯一词，即词频矩阵中所有出现的那些的唯一的单词\n",
    "# vectorizer.get_feature_names()  # 已经降级，使用 get_feature_names_out 替换\n",
    "vectorizer.get_feature_names_out()\n",
    "# 稀疏矩阵的输出方式：\n",
    "print(vectors)\n",
    "# 原始矩阵的输出方式：\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e950287-6ccd-4d0b-aec5-60d051ea5853",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. 统计每个词语的tf-idf权值\n",
    "参考：  https://zhuanlan.zhihu.com/p/166636681 、 https://www.ruanyifeng.com/blog/2013/03/tf-idf.html  \n",
    "tf：词频，某个词语在整篇文章中出现次数，一般除以整篇文章的词语总数，让tf在（0~1）之间，即所谓的归一化。  \n",
    "idf：逆词频，所有文章中包含某个词语的频率，用所有文章数除以包含这个词语的文章数表征，一般使用公式：log(总文章数/含有某个词语的文章数+1)。其中：  \n",
    "分母+1，防止没有包含某词语的文章的情况，此时含有某个词语的文章数为0，为了保证分母不能为0，故+1  \n",
    "使用log函数，一般是借助log函数的单调增函数的性质，缩小结果数据的绝对数值（参考：https://zhuanlan.zhihu.com/p/106232513 ），因为本例中总文章只有四篇，但是在实际应用中，总文章数应该是现实世界中的所有文章（能找到的），数量会很大，使用log主要的目的就是降低结果数据的值。\n",
    "**想想稀疏矩阵的性质，按行、列求和，都是是什么意思？**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8dcb90b-67f5-4f67-bad7-f3b71e1b0d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.6676785446095399\n",
      "  (0, 5)\t0.5264054336099155\n",
      "  (0, 1)\t0.5264054336099155\n",
      "  (1, 11)\t0.5254727492640658\n",
      "  (1, 6)\t0.5254727492640658\n",
      "  (1, 5)\t0.41428875116588965\n",
      "  (1, 2)\t0.5254727492640658\n",
      "  (2, 10)\t0.4472135954999579\n",
      "  (2, 9)\t0.4472135954999579\n",
      "  (2, 7)\t0.4472135954999579\n",
      "  (2, 4)\t0.4472135954999579\n",
      "  (2, 0)\t0.4472135954999579\n",
      "  (3, 3)\t0.7852882757103967\n",
      "  (3, 1)\t0.6191302964899972\n",
      "[[0.         0.52640543 0.         0.         0.         0.52640543\n",
      "  0.         0.         0.66767854 0.         0.         0.        ]\n",
      " [0.         0.         0.52547275 0.         0.         0.41428875\n",
      "  0.52547275 0.         0.         0.         0.         0.52547275]\n",
      " [0.4472136  0.         0.         0.         0.4472136  0.\n",
      "  0.         0.4472136  0.         0.4472136  0.4472136  0.        ]\n",
      " [0.         0.6191303  0.         0.78528828 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 该类会统计每个词语的tf-idf权值\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(vectors)\n",
    "# 输出tfidf的类型\n",
    "tfidf\n",
    "# 输出tfidf的稀疏矩阵形式，并按每篇文章（每句话）中的每个词语的tf-idf的权值逆序排序\n",
    "print(tfidf)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843c0a1-e8a9-4d1e-9b6c-aa40e4dcdbee",
   "metadata": {},
   "source": [
    "## 4. 输出相关词频、权重信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c46ae-d12c-4263-89de-1f6c6b79444f",
   "metadata": {},
   "source": [
    "### 4.1 输出所有的非停用词的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5a769c-9def-4c4c-8762-6d29d88e7b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\Python\\venvs\\work\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['中国', '北京', '大厦', '天安门', '小明', '来到', '杭研', '毕业', '清华大学', '硕士', '科学院', '网易']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词袋模型(vectorizer)中的所有词语\n",
    "# word = vectorizer.get_feature_names_out()\n",
    "word = vectorizer.get_feature_names()  # 这个看的更清晰\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ee57be-7e97-49e5-92a4-430ee2f4216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.52640543, 0.        , 0.        , 0.        ,\n",
       "        0.52640543, 0.        , 0.        , 0.66767854, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.52547275, 0.        , 0.        ,\n",
       "        0.41428875, 0.52547275, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.52547275],\n",
       "       [0.4472136 , 0.        , 0.        , 0.        , 0.4472136 ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.4472136 ,\n",
       "        0.4472136 , 0.        ],\n",
       "       [0.        , 0.6191303 , 0.        , 0.78528828, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将tf-idf矩阵抽取出来，元素a[i][j]（即：一篇文章中某个词语）表示j词在i类文本中的tf-idf权重\n",
    "weight = tfidf.toarray() \n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a86948-dc9b-45a5-af0c-c3e45ed630b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------这里输出第 0 类文本的词语tf-idf权重------\n",
      "中国 0.0\n",
      "北京 0.5264054336099155\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.5264054336099155\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.6676785446095399\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.0\n",
      "-------这里输出第 1 类文本的词语tf-idf权重------\n",
      "中国 0.0\n",
      "北京 0.0\n",
      "大厦 0.5254727492640658\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.41428875116588965\n",
      "杭研 0.5254727492640658\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.5254727492640658\n",
      "-------这里输出第 2 类文本的词语tf-idf权重------\n",
      "中国 0.4472135954999579\n",
      "北京 0.0\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.4472135954999579\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.4472135954999579\n",
      "清华大学 0.0\n",
      "硕士 0.4472135954999579\n",
      "科学院 0.4472135954999579\n",
      "网易 0.0\n",
      "-------这里输出第 3 类文本的词语tf-idf权重------\n",
      "中国 0.0\n",
      "北京 0.6191302964899972\n",
      "大厦 0.0\n",
      "天安门 0.7852882757103967\n",
      "小明 0.0\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "科学院 0.0\n",
      "网易 0.0\n"
     ]
    }
   ],
   "source": [
    "# 打印每篇文章中每个单词的tf-idf权重\n",
    "# 第一个for遍历所有文本，第二个for遍历某一类文本下的词语权重\n",
    "for i in range(len(weight)):  \n",
    "    print(\"-------这里输出第\", i, \"类文本的词语tf-idf权重------\")  \n",
    "    for j in range(len(word)):  \n",
    "        print(word[j], weight[i][j])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf5b24-1fea-4bf0-be86-326d51868fbe",
   "metadata": {},
   "source": [
    "# 以上！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
